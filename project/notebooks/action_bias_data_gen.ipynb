{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "gUGGN3x3b6yU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM was more willing to increase the bias levels when it was stated the goal was to predict bias in order to root it out. And also potentially when asking to awnser what you would expect not what should be the case. And making sure to mention in each question what you would predict in a \"action\" image dataset.\n",
        "\n",
        "gpt4 also provided more consistent outputs with this prompt than gpt-4o."
      ],
      "metadata": {
        "id": "DG5gTNVYolwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CreatePrompt(action):\n",
        "  action = action.replace('_', \" \").replace('-', \" \")\n",
        "  task = \"I am trying to predict what actions in a image or video dataset could have demographic biases in the samples.\"\n",
        "  goal = \"The idea is that with your help, we can know what actions to check to make sure they are not biased as predicted.\"\n",
        "  q1 = \"How much gender bias would you expect in a {} image dataset?\".format(action)\n",
        "  q2 = \"How much age bias would you expect in a {} image dataset?\".format(action)\n",
        "  q3 = \"How much race bias would you expect in a {} image dataset?\".format(action)\n",
        "  options = \"Options are none, small, medium, and large. Only respond to this prompt with the chosen option for each question separated by commas.\"\n",
        "  explain = \"After providing all of the answers, in a new line explain your reasoning.\"\n",
        "  return task + goal + q1 + q2 + q3 + options + explain"
      ],
      "metadata": {
        "id": "NQwpQXJQeucC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparation"
      ],
      "metadata": {
        "id": "iPlNjr0ixhpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = # key here\n",
        "client = OpenAI(api_key=key)\n",
        "bias_data_path = # bias data path here\n",
        "df = pd.read_csv(bias_data_path)\n",
        "actions = df['action'].unique()"
      ],
      "metadata": {
        "id": "jh2yZedJcnW1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "0626c822-5b02-4b2c-f504-de3a6866146d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-5-e39a8c30290a>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-e39a8c30290a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    key = # key here\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = []\n",
        "for action in actions:\n",
        "  prompts.append(CreatePrompt(action))\n",
        "\n",
        "# Open the batch file for writing\n",
        "with open(\"requests.jsonl\", \"w\") as f:\n",
        "    index = 0\n",
        "    for prompt in prompts:\n",
        "        request_payload = {\n",
        "            \"custom_id\": \"sample\" + str(index),\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"gpt-4\",\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "        index = index + 1\n",
        "        f.write(json.dumps(request_payload) + \"\\n\")\n",
        "\n",
        "print(\"âœ… Batch request file 'requests.jsonl' created.\")"
      ],
      "metadata": {
        "id": "2iSL3sqimw67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Prompt"
      ],
      "metadata": {
        "id": "CZFtzLGixb_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts[345]"
      ],
      "metadata": {
        "id": "3rCPk1yWd1vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with 1 Sample"
      ],
      "metadata": {
        "id": "m2W4xZN3xmVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"developer\",\n",
        "            \"content\": prompts[345]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "ILNa7Cfrg17U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Batch"
      ],
      "metadata": {
        "id": "3B0F6OWAxoRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_input_file = client.files.create(\n",
        "    file=open(\"requests.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")"
      ],
      "metadata": {
        "id": "VUoZOgI2ixuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Id"
      ],
      "metadata": {
        "id": "lw7ef1fyxtzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_input_file.id"
      ],
      "metadata": {
        "id": "tNjvKUBGix4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Batch"
      ],
      "metadata": {
        "id": "WF9gLSCdxwY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.batches.create(\n",
        "    input_file_id=batch_input_file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"inference\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Vx0NzYuYj_G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve Results\n",
        "\n",
        "Use the ID output in the create batch cell"
      ],
      "metadata": {
        "id": "yCHqJEw_yEV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id = # batch id here\n",
        "batch = client.batches.retrieve(id)\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "FRJ5StR4jDu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Results\n",
        "\n",
        "Use the output_file_id printed in the retreive retrieve results cell"
      ],
      "metadata": {
        "id": "Qpvq7BPiyCmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_id = # output file id here\n",
        "result = client.files.content(output_file_id).content"
      ],
      "metadata": {
        "id": "5BoAxRjuxTwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write To File"
      ],
      "metadata": {
        "id": "kWBUOR4OyGv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_file_name = \"batch.jsonl\"\n",
        "\n",
        "with open(result_file_name, 'wb') as file:\n",
        "    file.write(result)"
      ],
      "metadata": {
        "id": "w08ZbtrlxZs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data from saved file\n",
        "results = []\n",
        "with open(result_file_name, 'r') as file:\n",
        "    for line in file:\n",
        "        # Parsing the JSON string into a dict and appending to the list of results\n",
        "        json_object = json.loads(line.strip())\n",
        "        results.append(json_object)"
      ],
      "metadata": {
        "id": "DqCSf1fLxowR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Data"
      ],
      "metadata": {
        "id": "hisBG81gyJGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "final_data = []\n",
        "for res in results:\n",
        "    result = res['response']['body']['choices'][0]['message']['content']\n",
        "\n",
        "    # get data\n",
        "    lines = result.split('\\n')\n",
        "    first_line = lines[0].strip().replace('.', '')\n",
        "    biases = [word.strip().lower() for word in first_line.split(',')]\n",
        "\n",
        "    # create and add data\n",
        "    sample_data = {'action': actions[index], 'gender_bias': biases[0], 'age_bias': biases[1], 'race_bias': biases[2], 'reasoning': lines[2]}\n",
        "    final_data.append(sample_data)\n",
        "    index = index + 1\n",
        "dfb = pd.DataFrame(final_data)\n",
        "dfb.to_csv('bias_prediction_data.csv', index=False)"
      ],
      "metadata": {
        "id": "DOup36K6xrm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('bias_prediction_data.csv')"
      ],
      "metadata": {
        "id": "0Uwo4lp5oulC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Results"
      ],
      "metadata": {
        "id": "A2cg1tb2ygHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of choices\n",
        "dfp = data.drop(columns=['action', 'reasoning'])\n",
        "sns.countplot(data=dfp.melt(value_vars=dfp.columns), x='value', hue='variable', palette='Set2')\n",
        "\n",
        "# Titles and labels\n",
        "plt.title('GPT4 Bias Prediction for actions in HAA500 dataset')\n",
        "plt.xlabel('Predicted Bias')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UShcX1vVsAF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}